---
title: "Divvy Case Study â€” Process Phase (Data Cleaning)"
author: "Emmanuel Akinbile"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
---

# 1. Introduction

## 1.1 Business context (ASK)
This case study prepares and cleans Divvy ride data to enable analysis of rider behavior by segment.  
**Business task:** create a single, reliable dataset (Divvy **2019 Q1** + **2020 Q1**) suitable for comparing **member** vs **casual** usage patterns and informing strategies to increase conversions to membership.

**Primary questions for later analysis (not answered here):**
- How do ride duration, time-of-day, day-of-week, and station patterns differ between members and casual riders?
- Which differences persist across years (season-over-season for Q1)?
- What insights could inform targeted membership offers?

## 1.2 Audience & stakeholders
- Marketing / Growth team (decision-makers for membership strategy)  
- Operations (capacity, station placement, service hours)  
- Data / Analytics team (owns data pipelines and BI reporting)

## 1.3 Scope for this notebook
- **Phase:** Process (data cleaning and preparation).  
- **Files:** Divvy 2019 Q1 and 2020 Q1 CSV extracts.  
- **Output:** one tidy combined dataset with consistent schema, validated timestamps, derived ride length, and basic date/time features.

> Note: Using **Q1 only** introduces seasonality; results should not be generalized to a full year without additional data.

## 1.4 Data overview (pre-clean)
- **2019 Q1**: ~350k rows (legacy schema)  
- **2020 Q1**: ~427k rows (new schema)  
- Common entities: ride timestamps, start/end stations, member type.  
- Known differences: column names and availability (e.g., `usertype` in 2019 vs `member_casual` in 2020; `rideable_type` missing in 2019).

## 1.5 Assumptions & constraints
- Timezone assumed **America/Chicago** unless specified in source metadata.  
- Remove non-positive and extreme ride durations (> 24 hours).  
- Retain rows even if lat/long are missing (document missingness).  
- Station names normalized for whitespace only (no aggressive de-duplication).

## 1.6 Success criteria (for this phase)
- One combined dataset with:
  - Harmonized column names and types.  
  - Valid `started_at`, `ended_at`, and **ride_length_min**.  
  - Documented filters and row counts before/after cleaning.  
  - Reproducible code and a clear data dictionary.

## 1.7 Deliverables (from this notebook)
- **Clean dataset:** `divvy_cleaned.csv` (and `.rds`).  
- **Summary note:** pre/post row counts, % removed, missingness snapshot.  
- **Data dictionary:** final fields and definitions for downstream analysis.

---

## Load Required Libraries

```{r setup, message=FALSE, warning=FALSE}
# Load required libraries
library(tidyverse)   # data wrangling and visualization
library(lubridate)   # dates and times
library(janitor)     # cleaning column names
library(skimr)       # quick summaries
library(readr)       # CSV import
library(DT)          # interactive tables
library(knitr)       # pretty tables
```

## Import Raw Datasets

```{r import-data, message=FALSE, warning=FALSE}
# Import Divvy datasets
divvy_2019_q1 <- read_csv("Divvy_Trips_2019_Q1.csv")
divvy_2020_q1 <- read_csv("Divvy_Trips_2020_Q1.csv")

# Preview the first few rows of each dataset
head(divvy_2019_q1)
head(divvy_2020_q1)

# Check the dimensions (rows and columns)
dim(divvy_2019_q1)
dim(divvy_2020_q1)
```

## Prepare Datasets for Binding
```{r rename-columns, message=FALSE, warning=FALSE}
divvy_2019_q1 <- divvy_2019_q1 %>% 
  rename(
    ride_id = trip_id,
    started_at = start_time,
    ended_at = end_time,
    start_station_name = from_station_name,
    start_station_id = from_station_id,
    end_station_name = to_station_name,
    end_station_id = to_station_id,
    member_casual = usertype
      ) %>% 
  mutate(rideable_type = "classic_bike",
         start_lat = "NA",
         start_lng = "NA",
         end_lat   = "NA",
         end_lng   = "NA",
         member_casual="NA"
         )

# Make sure 2019 dataset columns match 2020 dataset order
divvy_2019_q1 <- divvy_2019_q1 %>%
  select(ride_id,
         rideable_type,
         started_at,
         ended_at,
         start_station_name,
         start_station_id,
         end_station_name,
         end_station_id,
         start_lat,
         start_lng,
         end_lat,
         end_lng,
         member_casual)

# Check if column names now match
all(names(divvy_2019_q1) == names(divvy_2020_q1))

```

## Standardizing Datatypes 
Here we will convert the datatypes in the 2019 dataset to match with the datatypes in the 2020 dataset.
```{r binding-datasets, message=FALSE, warning=FALSE}
str(divvy_2019_q1)##checking the datatypes of columns in 2019 data
str(divvy_2020_q1)##checking the datatypes of columns in 2020 data
sapply(divvy_2019_q1, class)
sapply(divvy_2020_q1, class)


# Match ride_id
divvy_2019_q1$ride_id <- as.character(divvy_2019_q1$ride_id)

# Match start and end times
divvy_2019_q1$started_at <- as.POSIXct(divvy_2019_q1$started_at,
                                             format = "%Y-%m-%d %H:%M:%S",
                                             tz = "UTC")
divvy_2019_q1$ended_at <- as.POSIXct(divvy_2019_q1$ended_at,
                                           format = "%Y-%m-%d %H:%M:%S",
                                           tz = "UTC")
divvy_2020_q1$started_at <- as.POSIXct(divvy_2020_q1$started_at,
                                             format = "%Y-%m-%d %H:%M:%S",
                                             tz = "UTC")
divvy_2020_q1$ended_at <- as.POSIXct(divvy_2020_q1$ended_at,
                                             format = "%Y-%m-%d %H:%M:%S",
                                             tz = "UTC")

# Match latitude/longitude
divvy_2019_q1$start_lat <- as.numeric(divvy_2019_q1$start_lat)
divvy_2019_q1$start_lng <- as.numeric(divvy_2019_q1$start_lng)
divvy_2019_q1$end_lat   <- as.numeric(divvy_2019_q1$end_lat)
divvy_2019_q1$end_lng   <- as.numeric(divvy_2019_q1$end_lng)

```

## Binding Datasets
Now we will bind the 2 datasets 

```{r binding-rows, message=FALSE, warning=FALSE}
all_trips <- bind_rows(divvy_2019_q1, divvy_2020_q1)
head(all_trips)
```

## Quality Checks 

```{r}
str(all_trips)
colSums(is.na(all_trips))
```
## Removing Unneeded Data
Now we will remove columns that won't be needed for analysis. After doing that, we recheck the columns for empty spaces and duplicate records. After selecting only dsitinct records and rechecking the number of rows, we realize there are no duplicate records.

```{r clean-columns, message=FALSE, warning=FALSE}
# Select only columns necessary for analysis
all_trips <- all_trips %>% 
  select(
    ride_id, rideable_type, started_at, ended_at,
         start_station_name, end_station_name, member_casual
  )
head(all_trips)
# Check for NA in the columns
colSums(is.na(all_trips))
# Check number of rows before removing duplicates
nrow(all_trips)

# Remove duplicates
all_trips <- all_trips %>% distinct()

# Check number of rows after
nrow(all_trips)

```
### Summary of Data Cleaning in RStudio  

```{r export-data, message=FALSE, warning=FALSE}
write.csv(all_trips, "divvy_cleaned.csv", row.names = FALSE)

```

In this process phase, the 2019 Q1 and 2020 Q1 Divvy trip datasets were imported into RStudio. The 2019 dataset was adjusted to align with the 2020 structure by renaming columns and adding a placeholder `rideable_type` field. Column order and data types were standardized to ensure compatibility. Both datasets were then combined into a single table using `bind_rows()`. Unnecessary columns were removed to simplify the dataset, and a duplicate check confirmed that no duplicate entries existed. The cleaned dataset is now ready for export to Google Sheets or Excel, where calculated fields such as `ride_length` and `day_of_week` will be added for further analysis.  

